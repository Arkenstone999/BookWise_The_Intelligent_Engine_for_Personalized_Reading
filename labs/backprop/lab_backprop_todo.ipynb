{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLQPJbJLlBq7"
      },
      "source": [
        "# Automatic Differentiation with PyTorch\n",
        "Created by [Santiago Pascual](https://scholar.google.es/citations?user=7cVOyh0AAAAJ&hl=ca) ([UPC School](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) 2019)\n",
        "\n",
        "Updated by [Xavier Giro](https://imatge.upc.edu/web/people/xavier-giro) ([UPC TelecomBCN](https://telecombcn-dl.github.io/dlai-2019/) 2019) and [Gerard I. GÃ¡llego](https://www.linkedin.com/in/gerard-gallego/)\n",
        "\n",
        "## Course material\n",
        "* [Slides](https://www.slideshare.net/xavigiro/backpropagation-for-neural-networks) by [Xavier Giro](https://imatge.upc.edu/web/people/xavier-giro)\n",
        "* [Video](https://www.youtube.com/watch?v=uub_hqDlqjc) by [Elisa Sayrol](https://imatge.upc.edu/web/people/elisa-sayrol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6S_xlm3AI05"
      },
      "source": [
        "This session will be about how to perform backpropagation in PyTorch. To build neural networks with PyTorch we must first understand how this framework simplifies our life. Central to all neural networks in PyTorch is the `autograd` package [1]. This package provides **automatic differentiation for all operations on Tensors**. *HOW COOL IS THAT?*\n",
        "\n",
        "This means you can put layers and layers of operations over your PyTorch tensors, and the `autograd` package already computes the derivatives of those operations in the backprop process. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different. *Wow wow, wait... define-by-run? every single iteration can be different? What is all this jargon??* Let's see the following concepts during this tutorial:\n",
        "\n",
        "* The `grad`s in our `tensor`s.\n",
        "* The dynamic computational graph concept (DCG).\n",
        "* The `.backward()` life saver."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XmDEbCMdGpO"
      },
      "source": [
        "# Imports that will be needed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2IIIbEwUV0C"
      },
      "source": [
        "### Long Story Short\n",
        "\n",
        "The `torch.Tensor` class has an attribute `.requires_grad`. If you set it to `True`, it **starts tracking all operations on it**. When you finish your computations you can call `.backward()` and have **all the gradients computed automatically**. The gradient for this tensor will be **accumulated** into the `.grad` attribute.\n",
        "\n",
        "*IMPORTANT: Accumulated means it sums up the new gradients to the already existing ones (if any)!*\n",
        "\n",
        "Any operation performed on a `Tensor` that conforms a `Function` (`torch.autograd.Function`) creates a new node of an acyclic graph. This means that each `Tensor` where it comes from (its source `Tensor` and the `Function` that created itself). The `Tensor` attribute `grad_fn` references the `Function` that created it. THAT SIMPLE.\n",
        "\n",
        "Example multiplication of two tensors and the resulting interconnections [2]:\n",
        "\n",
        "![img](https://miro.medium.com/max/336/1*jGo_2J9UQeynwG_3olUD4w.png)\n",
        "\n",
        "Well this is the so called *dynamic computational graph (DCG)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbGvYeNdljpR"
      },
      "source": [
        "#### Creating a tensor, operating on it, and computing derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQgEVpXkdGpQ"
      },
      "source": [
        "import torch\n",
        "\n",
        "def describe_tensor(tensor, name=''):\n",
        "  # Helper function to explore the attributes of a tensor object\n",
        "  print('-' * 30)\n",
        "  print('Name: ', name)\n",
        "  print('-' * 30)\n",
        "  print('data : ', tensor.data)\n",
        "  print('requires_grad : ', tensor.requires_grad)\n",
        "  print('grad: ', tensor.grad)\n",
        "  print('grad_fn: ', tensor.grad_fn)\n",
        "  print('is_leaf: ', tensor.is_leaf)\n",
        "  print('=' * 30)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYLZ9in3dGpR",
        "outputId": "3d8c7ea6-e780-42d5-c16b-a19d95329415"
      },
      "source": [
        "# create a tensor x\n",
        "x = torch.tensor(1.0)\n",
        "# create a tensor y\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "describe_tensor(x, name='x')\n",
        "describe_tensor(y, name='y')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  x\n",
            "------------------------------\n",
            "data :  tensor(1.)\n",
            "requires_grad :  False\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "------------------------------\n",
            "Name:  y\n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  False\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2sexkmKdGpS",
        "outputId": "aef357b2-fb3e-44a4-de36-a5357db512e1"
      },
      "source": [
        "# Create z as the multiplicative outcome of x * y\n",
        "z = x * y\n",
        "describe_tensor(z, name='z')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  z\n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  False\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soQlbc0IHH7a"
      },
      "source": [
        "We have created a DCG out of a simple product of two scalar tensors. But there is no node requiring gradients! Let's call the life saver `backward()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "4Fs4jL-PdGpT",
        "outputId": "41b1714b-50eb-4385-a120-c44f8fdc0f54"
      },
      "source": [
        "z.backward()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-40c0c9b0bbab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYMY1ke3ejl3"
      },
      "source": [
        "No hesitation, is was meant to crash. There is no `Tensor` requiring to track the graph because none required the gradients to be computed with `requires_grad=True`. Now we can make it require the gradients by simply using the inplace function `.require_gradients_(True)` or by specifying the flag as `True` at `Tensor` creation time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i--n7QFdGpU",
        "outputId": "3f13ce9b-254b-4262-f777-9cce11545436"
      },
      "source": [
        "# make x track gradients\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "z = x * y\n",
        "describe_tensor(z)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  \n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  True\n",
            "grad:  None\n",
            "grad_fn:  <MulBackward0 object at 0x7a169ecf8280>\n",
            "is_leaf:  False\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-377bf9e939b1>:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print('grad: ', tensor.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A60YjCAse8yQ"
      },
      "source": [
        "#### Note the change in the Tensor description!\n",
        "\n",
        "Now there are two important differences from this `z` to the previous one. First, this one DOES require gradient tracking. But secondly, it contains a `grad_fn` reference to a `MulBackward` operation! Which is basically telling us that multiplication will go through a derivative process in the backward step when we call the `backward()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ixYGvWQdGpV",
        "outputId": "e74f33b1-493a-40c5-bdb1-7443973906e5"
      },
      "source": [
        "# call .backward() now on z\n",
        "z.backward()\n",
        "\n",
        "# Now describe each tensor x, y and z\n",
        "describe_tensor(x, 'x')\n",
        "describe_tensor(y, 'y')\n",
        "describe_tensor(z, 'z')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  x\n",
            "------------------------------\n",
            "data :  tensor(1.)\n",
            "requires_grad :  True\n",
            "grad:  tensor(2.)\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "------------------------------\n",
            "Name:  y\n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  False\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "------------------------------\n",
            "Name:  z\n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  True\n",
            "grad:  None\n",
            "grad_fn:  <MulBackward0 object at 0x7a169ecfbac0>\n",
            "is_leaf:  False\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-377bf9e939b1>:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print('grad: ', tensor.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDt1X6nKh1up"
      },
      "source": [
        "Note that the cell above is showing the text description of the graph [2]:\n",
        "\n",
        "![img](https://miro.medium.com/max/471/1*viCEZbSODfA8ZA4ECPwHxQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmJ3x95wfhlA"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Where does the result of `x.grad` come from and why does it have this value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK_S-fY3rHD6"
      },
      "source": [
        "#### If you try `z.backward()` it will crash, notice the message"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGQKjaWldGpW"
      },
      "source": [
        "z.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAyOlfD6sMDM"
      },
      "source": [
        "#### When the backward computation is done...\n",
        "\n",
        "The DCG is removed, and so we cannot perform backprop anymore. Unless you specify you want to retain the graph to do as many backwards as desired (for whatever reason and for an advanced usage of PyTorch).\n",
        "\n",
        "### Exercise 2\n",
        "\n",
        "Make use of the `retain_graph` flag in the `backward` call to backpropagate twice a tensor of ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZwzykgndGpX"
      },
      "source": [
        "# Let's try to backward twice\n",
        "\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0)\n",
        "z = x * y\n",
        "\n",
        "# TODO: Backward twice\n",
        "z.backward(...)\n",
        "\n",
        "z.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5-ZoEsbvA6I"
      },
      "source": [
        "## Building a Neural Network and Training it\n",
        "\n",
        "We will now build a neural network to exemplify the simplicity of using PyTorch for deep learning. And then we will see how backpropagation is applied on it. The network will have one hidden layer and one output layer. We use the `nn` package in PyTorch to get to the neural components, also called `Module`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9myS_9XdGpY"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__() # must call the superclass init first\n",
        "    # First fully-connected layer (3 inputs, 20 hidden neurons)\n",
        "    self.fc1 = nn.Linear(3, 20)\n",
        "    # First hidden activation\n",
        "    self.act1 = nn.Tanh()\n",
        "    # Second fully-connected layer (20 hidden neurons, 3 outputs)\n",
        "    self.fc2 = nn.Linear(20, 3)\n",
        "    # No activation as we make it a linear output\n",
        "\n",
        "  def forward(self, x):\n",
        "    # activation of first layer is Tanh(FC1(x))\n",
        "    h1 = self.act1(self.fc1(x))\n",
        "    # output activation\n",
        "    y = self.fc2(h1)\n",
        "    return y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1OEsuwUdGpY",
        "outputId": "f93e79e3-243a-4108-a418-07a20ace7fd3"
      },
      "source": [
        "# We instantiate our network now, and can even print its structure\n",
        "net = MyNet()\n",
        "print(net)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyNet(\n",
            "  (fc1): Linear(in_features=3, out_features=20, bias=True)\n",
            "  (act1): Tanh()\n",
            "  (fc2): Linear(in_features=20, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v72a4zCbdGpY",
        "outputId": "ddd5c704-e4f5-40b8-8a8a-e939afdba7e1"
      },
      "source": [
        "# We can explore the weight tensor of a layer very simply\n",
        "describe_tensor(net.fc1.weight, 'FC1 weight')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  FC1 weight\n",
            "------------------------------\n",
            "data :  tensor([[ 0.0820, -0.2728, -0.4987],\n",
            "        [-0.1704, -0.0457, -0.2402],\n",
            "        [ 0.4759,  0.3915, -0.5022],\n",
            "        [-0.4966,  0.1689,  0.1641],\n",
            "        [-0.4144, -0.0208,  0.0228],\n",
            "        [-0.0547, -0.5132, -0.2290],\n",
            "        [-0.5337, -0.3761, -0.2014],\n",
            "        [ 0.0248, -0.0870,  0.4115],\n",
            "        [-0.1779, -0.2888,  0.2585],\n",
            "        [ 0.4950,  0.0872,  0.3694],\n",
            "        [ 0.0449, -0.1992,  0.1346],\n",
            "        [-0.4174, -0.4490, -0.0427],\n",
            "        [-0.1679, -0.3319,  0.0439],\n",
            "        [-0.4584,  0.3860, -0.1640],\n",
            "        [ 0.2005,  0.1846,  0.4600],\n",
            "        [-0.2692, -0.4379, -0.0448],\n",
            "        [-0.2663, -0.0878, -0.2732],\n",
            "        [-0.1209,  0.4165,  0.3313],\n",
            "        [ 0.3703, -0.0118, -0.1374],\n",
            "        [ 0.1427,  0.0252, -0.4192]])\n",
            "requires_grad :  True\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dToaGYVRxqcW"
      },
      "source": [
        "Observe that by default we have that the fully connected layer `fc1` DOES require the gradient computation. It is evident that they simplify our lives, because that is the last node to be reached in the backpropagation stage and we do not even have to take care of explicitly saying so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj6Zmx8jdGpZ",
        "outputId": "c1e63c26-302b-495e-f6d1-9df16032fed4"
      },
      "source": [
        "# We can access all the parameters of our network with the .parameters() function, that returns an iterable\n",
        "# over all tunnable params we created.\n",
        "params = list(net.parameters())\n",
        "for p in params:\n",
        "  print(p.shape)\n",
        "print('You should see two matrices (weights, OUTxIN) and two vectors (biases, OUT). Each pair of weight (W) and bias (b) comes from a fully connected layer.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 3])\n",
            "torch.Size([20])\n",
            "torch.Size([3, 20])\n",
            "torch.Size([3])\n",
            "You should see two matrices (weights, OUTxIN) and two vectors (biases, OUT). Each pair of weight (W) and bias (b) comes from a fully connected layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voBmJP_z0sTu"
      },
      "source": [
        "We will use mean squared error (MSE) as the loss function to be able to compute the error between our network outputs and some labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRBXLVSAdGpa"
      },
      "source": [
        "loss_fn = F.mse_loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-WeLqGhy80p"
      },
      "source": [
        "Now remember that training a neural network usually takes the following steps:\n",
        "\n",
        "1. Make a forward pass with some input `x` to activate each layer until output `y_`\n",
        "2. Compute the error towards a label `y` with a loss function (like MSE for example)\n",
        "3. Backpropagate the gradients through the network (`.backward()` call)\n",
        "4. Update every tunnable network parameter with its `.grad` attribute (using some optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIGbGkyi08dm"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "We will instantiate a network like the one shown earlier and train it to map simple uniform noise to zeros. We will track the loss value, which must decrease, and will plot it.\n",
        "\n",
        "1. **Understand why we call `optimizer.zero_grad()`** in the training loop. Read its functionallity in the PyTorch documentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer\n",
        "2. **Fill in the missing pieces to complete the aforementioned training steps** in order to observe a decreasing loss in the depicted plot. The loss should get very close to zero with a clear decreasing trend in few iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MK6zzHIedGpa",
        "outputId": "9c68e308-516f-4fbd-a350-ea343c985d6d"
      },
      "source": [
        "def train(network, optimizer, loss_fn, num_iters):\n",
        "    \"\"\" Training function \"\"\"\n",
        "    loss_history = []\n",
        "\n",
        "    for niter in range(1, num_iters + 1):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 1) Sample 10 random inputs of shape [10, 3]\n",
        "        x = torch.rand(10, 3)\n",
        "\n",
        "        # 2) Forward the data through the network\n",
        "        y_ = network(x)\n",
        "\n",
        "        # 3) Compute loss wrt zero labels\n",
        "        loss = loss_fn(y_, torch.zeros_like(y_))\n",
        "\n",
        "        # 4) Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Store the loss to plot\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        # 5) Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        if niter % 50 == 0:\n",
        "            print('Step {:2d} loss: {:.3f}'.format(niter, loss_history[-1]))\n",
        "\n",
        "    plt.plot(loss_history)\n",
        "    plt.xlabel('Niter')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "net = MyNet()\n",
        "# we will take stochastic gradient descent (SGD) to exemplify the training loop of a neural network\n",
        "# We first need to handle the parameters that the optimizer will tune, and then we must specify the learning rate (lr) of each\n",
        "# update step\n",
        "opt = optim.SGD(net.parameters(), lr=0.01)\n",
        "train(net, opt, loss_fn, 5000) #maybe we should iterate until finding the perfect number ? idk"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50 loss: 0.011\n",
            "Step 100 loss: 0.007\n",
            "Step 150 loss: 0.008\n",
            "Step 200 loss: 0.004\n",
            "Step 250 loss: 0.006\n",
            "Step 300 loss: 0.003\n",
            "Step 350 loss: 0.002\n",
            "Step 400 loss: 0.003\n",
            "Step 450 loss: 0.002\n",
            "Step 500 loss: 0.004\n",
            "Step 550 loss: 0.003\n",
            "Step 600 loss: 0.002\n",
            "Step 650 loss: 0.002\n",
            "Step 700 loss: 0.002\n",
            "Step 750 loss: 0.003\n",
            "Step 800 loss: 0.001\n",
            "Step 850 loss: 0.001\n",
            "Step 900 loss: 0.002\n",
            "Step 950 loss: 0.002\n",
            "Step 1000 loss: 0.001\n",
            "Step 1050 loss: 0.002\n",
            "Step 1100 loss: 0.002\n",
            "Step 1150 loss: 0.001\n",
            "Step 1200 loss: 0.001\n",
            "Step 1250 loss: 0.001\n",
            "Step 1300 loss: 0.001\n",
            "Step 1350 loss: 0.001\n",
            "Step 1400 loss: 0.001\n",
            "Step 1450 loss: 0.001\n",
            "Step 1500 loss: 0.000\n",
            "Step 1550 loss: 0.001\n",
            "Step 1600 loss: 0.001\n",
            "Step 1650 loss: 0.001\n",
            "Step 1700 loss: 0.000\n",
            "Step 1750 loss: 0.000\n",
            "Step 1800 loss: 0.000\n",
            "Step 1850 loss: 0.000\n",
            "Step 1900 loss: 0.000\n",
            "Step 1950 loss: 0.000\n",
            "Step 2000 loss: 0.001\n",
            "Step 2050 loss: 0.000\n",
            "Step 2100 loss: 0.000\n",
            "Step 2150 loss: 0.000\n",
            "Step 2200 loss: 0.000\n",
            "Step 2250 loss: 0.000\n",
            "Step 2300 loss: 0.000\n",
            "Step 2350 loss: 0.000\n",
            "Step 2400 loss: 0.000\n",
            "Step 2450 loss: 0.000\n",
            "Step 2500 loss: 0.000\n",
            "Step 2550 loss: 0.000\n",
            "Step 2600 loss: 0.000\n",
            "Step 2650 loss: 0.000\n",
            "Step 2700 loss: 0.000\n",
            "Step 2750 loss: 0.000\n",
            "Step 2800 loss: 0.000\n",
            "Step 2850 loss: 0.000\n",
            "Step 2900 loss: 0.000\n",
            "Step 2950 loss: 0.000\n",
            "Step 3000 loss: 0.000\n",
            "Step 3050 loss: 0.000\n",
            "Step 3100 loss: 0.000\n",
            "Step 3150 loss: 0.000\n",
            "Step 3200 loss: 0.000\n",
            "Step 3250 loss: 0.000\n",
            "Step 3300 loss: 0.000\n",
            "Step 3350 loss: 0.000\n",
            "Step 3400 loss: 0.000\n",
            "Step 3450 loss: 0.000\n",
            "Step 3500 loss: 0.000\n",
            "Step 3550 loss: 0.000\n",
            "Step 3600 loss: 0.000\n",
            "Step 3650 loss: 0.000\n",
            "Step 3700 loss: 0.000\n",
            "Step 3750 loss: 0.000\n",
            "Step 3800 loss: 0.000\n",
            "Step 3850 loss: 0.000\n",
            "Step 3900 loss: 0.000\n",
            "Step 3950 loss: 0.000\n",
            "Step 4000 loss: 0.000\n",
            "Step 4050 loss: 0.000\n",
            "Step 4100 loss: 0.000\n",
            "Step 4150 loss: 0.000\n",
            "Step 4200 loss: 0.000\n",
            "Step 4250 loss: 0.000\n",
            "Step 4300 loss: 0.000\n",
            "Step 4350 loss: 0.000\n",
            "Step 4400 loss: 0.000\n",
            "Step 4450 loss: 0.000\n",
            "Step 4500 loss: 0.000\n",
            "Step 4550 loss: 0.000\n",
            "Step 4600 loss: 0.000\n",
            "Step 4650 loss: 0.000\n",
            "Step 4700 loss: 0.000\n",
            "Step 4750 loss: 0.000\n",
            "Step 4800 loss: 0.000\n",
            "Step 4850 loss: 0.000\n",
            "Step 4900 loss: 0.000\n",
            "Step 4950 loss: 0.000\n",
            "Step 5000 loss: 0.000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA23afv_wJq9"
      },
      "source": [
        "We can compare the difference in output from a non-trained and the trained network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZhhXEo6dGpb"
      },
      "source": [
        "# Generate 5 random samples of dimensionality 3\n",
        "x_sample = torch.rand(5, 3)\n",
        "\n",
        "# Forward the data through a non trained network\n",
        "non_trained = MyNet()\n",
        "print('Non-trained result: ', torch.mean(non_trained(x_sample)).item())\n",
        "\n",
        "# Forward through the trained network (net)\n",
        "print('Trained result: ', torch.mean(net(x_sample)).item())\n",
        "\n",
        "print('Trained result should be closer to zero than the non-trained one (if training went well).')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY2OxOiH9ZQ2"
      },
      "source": [
        "### When we do NOT want gradients\n",
        "\n",
        "There are some scenarios where we want to avoid building the `backward` graph, as we will not need gradients. For example, during inference/prediction/test. We can avoid the computation of gradients through the neural network forward pass by enclosing it into the `with torch.no_grad()` context (**which speeds up evaluation process by x2 or x3 normally**). As an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLakZ2CFdGpc"
      },
      "source": [
        "x = torch.zeros(10, 3)\n",
        "with torch.no_grad():\n",
        "  y_ = net(x)\n",
        "  loss = loss_fn(y_, torch.zeros(x.shape))\n",
        "  print('Loss: {:.2f}'.format(loss))\n",
        "  describe_tensor(loss, 'loss')\n",
        "  print('NOTE THAT requires_grad=False NOW IN THE LOSS TENSOR')\n",
        "  # This would crash: y.backward()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNyoimhfACrG"
      },
      "source": [
        "Finally, we can also cut the graph at any point we want (if we want) with the `.detach()` function of a `Tensor`. For instance, if we only wanted to train the output layer in the previous network (leaving the first layer `fc1` to behave randomly for whatever reason), we can re-define it as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjItqNmZdGpc"
      },
      "source": [
        "class MyNetWithDetach(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__() # must call the superclass init first\n",
        "    # First fully-connected layer (3 inputs, 20 hidden neurons)\n",
        "    self.fc1 = nn.Linear(3, 20)\n",
        "    # First hidden activation\n",
        "    self.act1 = nn.Tanh()\n",
        "    # Second fully-connected layer (20 hidden neurons, 3 outputs)\n",
        "    self.fc2 = nn.Linear(20, 3)\n",
        "    # No activation as we make it a linear output\n",
        "\n",
        "  def forward(self, x):\n",
        "    # activation of first layer is Tanh(FC1(x))\n",
        "    h1 = self.act1(self.fc1(x))\n",
        "    # DETACH\n",
        "    h1 = h1.detach()\n",
        "    # output activation\n",
        "    y = self.fc2(h1)\n",
        "    return y\n",
        "\n",
        "# Now we can train this network\n",
        "net = MyNetWithDetach()\n",
        "# Now we can observe the difference of gradients in the biases of the 2 layers\n",
        "# between this network and the regular one\n",
        "# in terms of computed gradients\n",
        "\n",
        "def forward_backward(network, net_name=''):\n",
        "  x = torch.zeros(10, 3)\n",
        "  y_ = network(x)\n",
        "  loss = loss_fn(y_, torch.zeros(x.shape))\n",
        "  loss.backward()\n",
        "  describe_tensor(network.fc1.bias, '{}:FC1 bias'.format(net_name))\n",
        "  describe_tensor(network.fc2.bias, '{}:FC2 bias'.format(net_name))\n",
        "\n",
        "# Try with a non-detached network\n",
        "forward_backward(MyNet(), 'Non-Detached Net')\n",
        "# Try with a detached network\n",
        "forward_backward(MyNetWithDetach(), 'Detached Net')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-8_-TyB8Ju"
      },
      "source": [
        "And you may wonder...\n",
        "\n",
        "1. *Why would I cut the gradient flow at a certain point in my neural network? It looks like avoiding the learning process in some components. How may this be beneficial?* **A: Well, a neural network can be trained per blocks, as they are also tunnable feature extractors. Just bear in mind that you can bring a pre-trained neural network piece, attach it to your own additional piece, and tune only your own part of the network by freezing the first one**.\n",
        "\n",
        "2. *What happens to the optimizer, which has the full list of parameters of my network, after I detach the graph?* **A: the optimizer still contains a reference to your parameters. So in the MyNetWithDetach case, it still has a reference to fc1 parameters. Nonetheless, as `.grad` is None, it simply cannot update the parameters.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLjQHWuk06p"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
        "\n",
        "[2] https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95\n",
        "\n",
        "[3] https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html"
      ]
    }
  ]
}